{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ongoing-parameter",
   "metadata": {},
   "source": [
    "This Jupyter notebook contains a set of sample codes that are necessary for obtaining the calculation results of three different datasets as described in the paper titled \"Functional Output Regression for Machine Learning in Materials Science\". Readers can use the code to perform model training and test with full hyperparameter search, which will take a long time. Instead, readers can also test the performance of the best performing models that are downloadable from our github server. \n",
    "\n",
    "We provide three pre-trained models for each datasets.\n",
    "- Dataset 1: https://github.com/yoshida-lab/XenonPy/releases/download/v0.6.3/pretrained_models.zip\n",
    "- Dataset 2: https://github.com/yoshida-lab/XenonPy/releases/download/v0.6.3/dataset2.zip\n",
    "- USGS: https://github.com/yoshida-lab/XenonPy/releases/download/v0.6.3/USGS.zip\n",
    "\n",
    "Please update the variable \"model_dir\" in the \"User parameter setting\" section accordingly.\n",
    "\n",
    "To obtain the datasets used in this study, please access the following references.\n",
    "- For Dataset I and Dataset II:\n",
    "Urbina et al. (2021) UV-adVISor: Attention-Based Recurrent Neural Networks to Predict UVâ€“Vis Spectra. Analytical Chemistry, 93(48), 16076-16085.\n",
    "(https://pubs.acs.org/doi/10.1021/acs.analchem.1c03741)\n",
    "- USGS:\n",
    "Kokaly et al. (2017) USGS Spectral Library Version 7 Data: U.S. Geological Survey data release (https://dx.doi.org/10.5066/F7RR1WDJ)\n",
    "\n",
    "For Dataset I and II, please directly pick the files \"Datasets I, II, III spectra and SMILES/Dataset_I.csv\" and \"Datasets I, II, III spectra and SMILES/Dataset_II.csv\" in the downloaded zip file, respectively, as the input csv files. For USGS dataset, please process the downloaded data using \"process_dataset_usgs.ipynb\" (https://github.com/yoshida-lab/XenonPy/blob/master/samples/process_dataset_usgs.ipynb) to obtain the compact csv files used in our study. A file named \"Dataset_USGS.csv\" shall be produced after running the codes in the notebook. If there is any trouble processing the data, please contact us for help.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "rational-throw",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [23:24:15] Enabling RDKit 2019.09.1 jupyter extensions\n",
      "[23:24:15] Enabling RDKit 2019.09.1 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "# python packages used in the sample codes\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "from torch import optim\n",
    "\n",
    "from rdkit.Chem.Fingerprints import FingerprintMols\n",
    "from rdkit.Avalon import pyAvalonTools\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit import rdBase, Chem, DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem.AtomPairs import Pairs, Torsions\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# user-friendly print out\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-hepatitis",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-resource",
   "metadata": {},
   "source": [
    "#### Utility function: define RBFKernel class to be the radial basis function kernel that will be used in our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "horizontal-province",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from typing import Union, Sequence, Callable, Tuple\n",
    "\n",
    "\n",
    "class RBFKernel():\n",
    "    def __init__(self, sigmas_squared: Union[float, int, np.ndarray, Sequence], hight=10, *, dtype='float32'):\n",
    "        \"\"\"\n",
    "        Radial Basis Function (RBF) kernel function.\n",
    "        Ref: https://en.wikipedia.org/wiki/Radial_basis_function_kernel\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sigmas:\n",
    "            The squared standard deviations (SD).\n",
    "            Can be a single number or a 1d array-like object.\n",
    "        x_i: np.ndarray\n",
    "            Should be a 1d array.\n",
    "        x_j: np.ndarray\n",
    "            Should be a 1d array.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Distribution under RBF kernel.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            Raise error if sigmas has wrong dimension.\n",
    "        \"\"\"\n",
    "        sigmas_squared = np.asarray(sigmas_squared)\n",
    "        if sigmas_squared.ndim == 0:\n",
    "            sigmas_squared = sigmas_squared[np.newaxis]\n",
    "        if sigmas_squared.ndim != 1:\n",
    "            raise ValueError('parameter `sigmas_squared` must be a array-like object which has dimension 1')\n",
    "        self._sigmas_squared = sigmas_squared\n",
    "        self.hight = hight\n",
    "        self.dtype = dtype\n",
    "        \n",
    "    @property\n",
    "    def sigmas_squared(self):\n",
    "        return np.copy(self._sigmas_squared)\n",
    "    \n",
    "    def __call__(self, x_i: np.ndarray, x_j: Union[np.ndarray, int, float]):\n",
    "        # K(x_i, x_j) = exp(-||x_i - x_j||^2 / (2 * sigma^2))\n",
    "        p1 = np.power(np.expand_dims(x_i, axis=x_i.ndim) - x_j, 2)\n",
    "        p2 = self._sigmas_squared * 2\n",
    "        dists = self.hight * np.exp(-np.expand_dims(p1, axis=p1.ndim) / p2)\n",
    "\n",
    "        if dists.shape[2] == 1:\n",
    "            return dists[:, :, 0].astype(self.dtype)\n",
    "        return dists.astype(self.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "canadian-procurement",
   "metadata": {},
   "source": [
    "#### Model definition: define KernelNet class to be the neural network based kernel function model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caroline-final",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xenonpy.model import SequentialLinear\n",
    "\n",
    "# model parameter initialization\n",
    "@torch.no_grad()\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.BatchNorm1d):\n",
    "        nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        \n",
    "class KernelNet(nn.Module):\n",
    "    def __init__(self, *,\n",
    "                 n_neurons=(1024, 896, 512, 256, 128),\n",
    "                 activation_func=nn.LeakyReLU(0.2, inplace=True),\n",
    "                 kernel_grids: Union[int, np.ndarray] = 128,\n",
    "                 wavelength_points: Union[int, np.ndarray] =181,\n",
    "                 kernel_func=RBFKernel(sigmas_squared=0.05, hight=1),\n",
    "                ):\n",
    "        super().__init__()\n",
    "\n",
    "        # initializing fixed params (not updated through training)\n",
    "        if isinstance(kernel_grids, int):\n",
    "            kernel_grids = np.linspace(0, 1, kernel_grids)\n",
    "        if isinstance(kernel_grids, np.ndarray):\n",
    "            self.kernel_grids = kernel_grids\n",
    "        else:\n",
    "            raise ValueError('kernel_grids error!')\n",
    "        \n",
    "        if isinstance(wavelength_points, int):\n",
    "            wavelength_points = np.linspace(0, 1, wavelength_points)\n",
    "        if isinstance(wavelength_points, np.ndarray):\n",
    "            self.wavelength_points = wavelength_points\n",
    "        else:\n",
    "            raise ValueError('wavelength_points error!')\n",
    "        \n",
    "        # fully connected layers\n",
    "        self.fc_layers = SequentialLinear(\n",
    "            in_features=n_neurons[0], out_features=n_neurons[-1], h_neurons=n_neurons[1:-1], h_activation_funcs=activation_func\n",
    "        )\n",
    "        \n",
    "        # baseline parameters\n",
    "        self.mu = torch.nn.Parameter(torch.zeros(wavelength_points.size))\n",
    "        \n",
    "        # kernel\n",
    "        self.kernel = torch.from_numpy(\n",
    "            kernel_func(self.kernel_grids, self.wavelength_points)\n",
    "        )\n",
    "        \n",
    "    def to(self, *arg, **kwarg):\n",
    "        self.kernel = self.kernel.to(*arg, **kwarg)\n",
    "        return super().to(*arg, **kwarg)\n",
    "\n",
    "    def forward(self, fingerprint_input):\n",
    "        x = self.fc_layers(fingerprint_input)\n",
    "        x = x.view(x.size(0), self.kernel_grids.size, -1) \n",
    "        x = x * self.kernel\n",
    "        x = torch.sum(x, dim=1)\n",
    "        x = self.mu + x\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ac5ce3-c56c-4aff-84c7-8053095f70ee",
   "metadata": {},
   "source": [
    "# User parameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d814c7d-a2ea-4f54-a2d7-e4d26b2edac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform best case only or not: True - best model only; False - all hyperparameter search\n",
    "best_case = True\n",
    "\n",
    "# Target dataset: 1 - Dataset I; 2 - Dataset II; 3 - USGS\n",
    "data_case = 1\n",
    "\n",
    "# Full directory of the processed data file\n",
    "data_file = \"Dataset_I.csv\"\n",
    "\n",
    "# Directory and folder name for saving all the outputs\n",
    "out_directory = \"spectrum_results\"\n",
    "\n",
    "# Directory and folder name for saving or loading of models\n",
    "if data_case == 1:\n",
    "    model_dir = 'model/dataset1'\n",
    "elif data_case == 2:\n",
    "    model_dir = 'model/dataset2'\n",
    "elif data_case == 3:\n",
    "    model_dir = 'model/usgs'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34310578-9373-4f64-a34e-2564fa3b5443",
   "metadata": {},
   "source": [
    "# Main codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detected-swaziland",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "distant-driving",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of data: 949\n"
     ]
    }
   ],
   "source": [
    "# load csv file\n",
    "input_file = pd.read_csv(data_file,sep=\",\")\n",
    "\n",
    "# Figure\n",
    "out_result = Path(out_directory)\n",
    "out_result.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# drop \"OC(=O)C(=C\\C1=CC=[Cl]C=[Cl]1)\\C1=CN=CC=C1\" because of Explicit valence error (only relevant for Dataset I)\n",
    "input_file = input_file[input_file.SMILES != \"OC(=O)C(=C\\C1=CC=[Cl]C=[Cl]1)\\C1=CN=CC=C1\"]\n",
    "# drop data containing NANã€€because of Explicit valence error (only relevant for Dataset II)\n",
    "input_file = input_file.dropna()\n",
    "\n",
    "file_smiles_spectrum = input_file.values\n",
    "smiles_list = (file_smiles_spectrum[:,1])\n",
    "spectrum_list = file_smiles_spectrum[:,2:]\n",
    "spectrum_list = np.array(spectrum_list, dtype='float32')\n",
    "\n",
    "print(\"number of data:\", len(smiles_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-think",
   "metadata": {},
   "source": [
    "#### convert smiles to fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "considerable-scale",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([949, 1024])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([949, 182])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert smiles to mol file\n",
    "mol_list = [Chem.MolFromSmiles(s) for s in smiles_list if s is not None]\n",
    "    \n",
    "# Morgan fingerprint (ECFP) with radius=3 and 1024 bits\n",
    "fps_bit = [AllChem.GetMorganFingerprintAsBitVect(m, 3, 1024) for m in mol_list]\n",
    "fps_list = []\n",
    "for fps in fps_bit:\n",
    "    fps_arr = np.zeros((1,))\n",
    "    DataStructs.ConvertToNumpyArray(fps, fps_arr)\n",
    "    fps_list.append(fps_arr)\n",
    "fps_list = np.array(fps_list, dtype='float32')\n",
    "\n",
    "fps_list_max = np.amax(fps_list)\n",
    "fps_list_min = np.amin(fps_list)\n",
    "fps_norm = ((fps_list - fps_list_min) / (fps_list_max - fps_list_min))\n",
    "X_data = torch.from_numpy(fps_norm)\n",
    "\n",
    "# process spectrum data\n",
    "spectrum_list_max = np.amax(spectrum_list)\n",
    "spectrum_list_min = np.amin(spectrum_list)\n",
    "spectrum_norm = ((spectrum_list - spectrum_list_min) / (spectrum_list_max - spectrum_list_min))\n",
    "Y_data = torch.from_numpy(spectrum_norm)\n",
    "data_number = torch.zeros((X_data.shape[0],1))\n",
    "Y_data = torch.cat((data_number, Y_data), dim=1)\n",
    "for i in range(Y_data.shape[0]):\n",
    "    number = torch.tensor([i]) \n",
    "    Y_data[i][0] = number\n",
    "    \n",
    "# show data size\n",
    "X_data.shape\n",
    "Y_data.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-vessel",
   "metadata": {},
   "source": [
    "### Set up hyper-params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charming-reader",
   "metadata": {},
   "source": [
    "#### 1. model training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "tutorial-hearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 3000 # epoch\n",
    "lr = 0.0002 # learning rate\n",
    "ngpu = 2 # number of GPU\n",
    "display_interval = 100\n",
    "\n",
    "model_path = Path(model_dir)\n",
    "model_path.mkdir(exist_ok=True, parents=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "varying-marking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use GPU if available\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "# split train:val:test \n",
    "if data_case == 1:\n",
    "    split_size = 0.316 # DatasetI:0.316, DatasetII:0.301, USGS:0.2\n",
    "    split_val_size = 0.5 # DatasetI:0.5, DatasetII:0.488, USGS:0.5\n",
    "elif data_case == 2:\n",
    "    split_size = 0.301 # DatasetI:0.316, DatasetII:0.301, USGS:0.2\n",
    "    split_val_size = 0.488 # DatasetI:0.5, DatasetII:0.488, USGS:0.5\n",
    "elif data_case == 3:\n",
    "    split_size = 0.2 # DatasetI:0.316, DatasetII:0.301, USGS:0.2\n",
    "    split_val_size = 0.5 # DatasetI:0.5, DatasetII:0.488, USGS:0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbc29884-4885-4f06-bfbf-ed7bd6c7df4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexing corresponding to model parameters below\n",
    "if best_case:\n",
    "    if data_case == 1:\n",
    "        start_layer = 3\n",
    "        end_layer = 4\n",
    "        start_hyper = 7\n",
    "        end_hyper = 8\n",
    "        wavelength_points = 181\n",
    "        kernel_grids = 128\n",
    "    elif data_case == 2:\n",
    "        start_layer = 2\n",
    "        end_layer = 3\n",
    "        start_hyper = 7\n",
    "        end_hyper = 8\n",
    "        wavelength_points = 171\n",
    "        kernel_grids = 128\n",
    "    elif data_case == 3:\n",
    "        start_layer = 2\n",
    "        end_layer = 3\n",
    "        start_hyper = 6\n",
    "        end_hyper = 7\n",
    "        wavelength_points = 2151\n",
    "        kernel_grids = 256\n",
    "else:\n",
    "    start_layer = 0\n",
    "    end_layer = 4\n",
    "    start_hyper = 0\n",
    "    end_hyper = 12\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-smell",
   "metadata": {},
   "source": [
    "#### 2. model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sporting-intranet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of hidden layers\n",
    "if data_case == 3:\n",
    "    n_neurons = [\n",
    "        (1024, 640, 256),                 # DatasetUSGS: 1 hidden layer\n",
    "        (1024, 768, 512, 256),            # DatasetUSGS: 2 hidden layers\n",
    "        (1024, 832, 640, 448, 256),       # DatasetUSGS: 3 hidden layers\n",
    "        (1024, 873, 718, 564, 410, 256),  # DatasetUSGS: 4 hidden layers\n",
    "    ]\n",
    "else:\n",
    "    n_neurons = [\n",
    "        (1024, 512, 128),                 # DatasetI,II: 1 hidden layer\n",
    "        (1024, 512, 256, 128),            # DatasetI,II: 2 hidden layers\n",
    "        (1024, 896, 512, 256, 128),       # DatasetI,II: 3 hidden layers\n",
    "        (1024, 896, 640, 512, 256, 128),  # DatasetI,II: 4 hidden layers\n",
    "    ]\n",
    "    \n",
    "# the length and variance of RBF kernel\n",
    "kernel_hypers = (\n",
    "    [10, 0.00005], [5, 0.00005], [1, 0.00005], [0.5, 0.00005],\n",
    "    [10, 0.0005], [5, 0.0005], [1, 0.0005], [0.5, 0.0005],\n",
    "    [10, 0.00025], [5, 0.00025], [1, 0.00025], [0.5, 0.00025],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-egypt",
   "metadata": {},
   "source": [
    "### Begin model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "strategic-capitol",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9f08e9f9d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fixing random seeds for reproducing the results\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-recruitment",
   "metadata": {},
   "source": [
    "#### train with different hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changed-marshall",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (data_case == 1 or data_case == 2):\n",
    "    batch_number = 50\n",
    "elif (data_case == 3):\n",
    "    batch_number = 9\n",
    "\n",
    "# repeat for different number of layers in the neural network\n",
    "for layer_number in range(start_layer, end_layer):\n",
    "    # repeat for different sigma values for the kernel\n",
    "    for hyper_number in range(start_hyper, end_hyper):\n",
    "        # repeat for three times to check sensitivity to data splitting\n",
    "        for random_number in range(3):\n",
    "            # split data with different random seed\n",
    "            X_train, X_test_val, Y_train, Y_test_val = train_test_split(X_data, Y_data, test_size=split_size, random_state=random_number) \n",
    "            X_test, X_val, Y_test, Y_val = train_test_split(X_test_val, Y_test_val, test_size=split_val_size, random_state=random_number)\n",
    "            Dataset_train = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "            \n",
    "            loader_train = torch.utils.data.DataLoader(Dataset_train, batch_size=batch_number, shuffle=True)\n",
    "            Dataset_val = torch.utils.data.TensorDataset(X_val, Y_val)\n",
    "            loader_val = torch.utils.data.DataLoader(Dataset_val, shuffle=False)\n",
    "\n",
    "            # build model\n",
    "            netG = KernelNet(\n",
    "                n_neurons=n_neurons[layer_number],\n",
    "                wavelength_points=wavelength_points,\n",
    "                kernel_grids=kernel_grids,\n",
    "                kernel_func=RBFKernel(\n",
    "                    sigmas_squared=kernel_hypers[hyper_number][1],\n",
    "                    hight=kernel_hypers[hyper_number][0]\n",
    "                ),\n",
    "            ).to(device)\n",
    "\n",
    "            # Handle multi-gpu if desired\n",
    "\n",
    "            # if (device.type == 'cuda') and (torch.cuda.device_count() > ngpu):\n",
    "            #     netG = nn.DataParallel(netG, device_ids=range(ngpu))            \n",
    "            netG = netG.apply(weights_init)\n",
    "\n",
    "            # setup optimizer for training model\n",
    "            regu = torch.Tensor([1]).to(device) # parameter regularization\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.999), weight_decay=1e-5)\n",
    "            \n",
    "            # train with fixed number of epochs\n",
    "            losses = []\n",
    "            print(f\"training {layer_number + 1} hidden layer model with hyper_number {hyper_number + 1} and random_number {random_number}...\")\n",
    "            \n",
    "            for epoch in range(n_epoch):\n",
    "                for itr, data in enumerate(loader_train):\n",
    "                    real_fps = data[0][:,:].to(device)\n",
    "                    real_spectrum = data[1][:,1:].to(device)\n",
    "                    optimizerG.zero_grad()\n",
    "\n",
    "                    pred_spectrum = netG(real_fps)\n",
    "\n",
    "                    mu_parameter = netG.mu\n",
    "                    mu_para_diff = torch.diff(mu_parameter)\n",
    "                    delta_mu = torch.sum(torch.pow((mu_para_diff),2))\n",
    "                    \n",
    "                    loss = criterion(pred_spectrum, real_spectrum[:,:]) + regu * delta_mu\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizerG.step()\n",
    "                    losses.append(loss.item())\n",
    "\n",
    "                    if epoch % display_interval == 0 and itr % display_interval == 0:\n",
    "                        print('[{}/{}][{}/{}] Loss: {:.7f} '.format(epoch + 1, n_epoch, itr + 1, len(loader_train), loss.item()))\n",
    "                        \n",
    "            # save model\n",
    "            PATH_G = '{}/generator_time{:03d}_layer{:03d}_hyper{:03d}.pth'.format(model_path, random_number, layer_number+1, hyper_number+1) \n",
    "            torch.save(netG.state_dict(), PATH_G)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-disposal",
   "metadata": {},
   "source": [
    "### Test model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "civic-blocking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################\n",
      "##     output results  ##\n",
      "#####################\n",
      "KernelNet(\n",
      "  (fc_layers): SequentialLinear(\n",
      "    (layer_0): LinearLayer(\n",
      "      (linear): Linear(in_features=1024, out_features=896, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (normalizer): BatchNorm1d(896, eps=0.1, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (layer_1): LinearLayer(\n",
      "      (linear): Linear(in_features=896, out_features=640, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (normalizer): BatchNorm1d(640, eps=0.1, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (layer_2): LinearLayer(\n",
      "      (linear): Linear(in_features=640, out_features=512, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (normalizer): BatchNorm1d(512, eps=0.1, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (layer_3): LinearLayer(\n",
      "      (linear): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (normalizer): BatchNorm1d(256, eps=0.1, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (output): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      ")\n",
      "Median of RMSE: 0.124644004 std: 0.011839285\n",
      "Median of R square: 0.7629515670007748 std: 0.06191901874519955\n",
      "Median of MAE: 0.083352484 std: 0.0091531025\n",
      "Median of RMSE_derivative 0.013060271 std: 0.0008302324\n"
     ]
    }
   ],
   "source": [
    "# repeat for different number of layers in the neural network\n",
    "for layer_number in range(start_layer, end_layer):\n",
    "    # repeat for different sigma values for the kernel\n",
    "    for hyper_number in range(start_hyper, end_hyper):\n",
    "        # initializing variables for performance evaluation\n",
    "        anal_index = []        \n",
    "        anal_real_list = []\n",
    "        anal_pred_list = []\n",
    "\n",
    "        peak_position_real_list = []\n",
    "        peak_position_pred_list = []\n",
    "        peak_intensity_real_list = []\n",
    "        peak_intensity_pred_list = []\n",
    "\n",
    "        rmse_median = []\n",
    "        rmse_data = []\n",
    "        rsquare_median = []\n",
    "        mae_median = []\n",
    "        drmse_median = []\n",
    "        \n",
    "        # repeat for three times to check sensitivity to data splitting\n",
    "        for random_number in range(3):\n",
    "            # split data with different random seed\n",
    "            X_train, X_test_val, Y_train, Y_test_val = train_test_split(X_data, Y_data, test_size=split_size, random_state=random_number) \n",
    "            X_test, X_val, Y_test, Y_val = train_test_split(X_test_val, Y_test_val, test_size=split_val_size, random_state=random_number)\n",
    "            Dataset_train = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "            loader_train = torch.utils.data.DataLoader(Dataset_train, batch_size=50, shuffle=True)\n",
    "            Dataset_val = torch.utils.data.TensorDataset(X_val, Y_val)\n",
    "            loader_val = torch.utils.data.DataLoader(Dataset_val, shuffle=False)\n",
    "\n",
    "            # build model\n",
    "            netG = KernelNet(\n",
    "                n_neurons=n_neurons[layer_number],\n",
    "                wavelength_points=wavelength_points,\n",
    "                kernel_grids=kernel_grids,\n",
    "                kernel_func=RBFKernel(\n",
    "                    sigmas_squared=kernel_hypers[hyper_number][1],\n",
    "                    hight=kernel_hypers[hyper_number][0]\n",
    "                ),\n",
    "            ).to(device)\n",
    "    \n",
    "            #########\n",
    "            ## Load trained model\n",
    "            #########\n",
    "            trained_netG_path = '{}/generator_time{:03d}_layer{:03d}_hyper{:03d}.pth'.format(model_path, random_number, layer_number+1, hyper_number+1)\n",
    "            netG.load_state_dict(torch.load(trained_netG_path))     \n",
    "\n",
    "            ############\n",
    "            ### Analysis\n",
    "            ############\n",
    "            pred_spectrum = netG(X_test.to(device))\n",
    "            anal_real = Y_test[:,1:].to('cpu').detach().numpy().copy()\n",
    "            anal_pred = pred_spectrum[:].to('cpu').detach().numpy().copy()\n",
    "            anal_index.extend(Y_test[:,0].to('cpu').detach().numpy().copy())\n",
    "            anal_real_list.extend(anal_real)\n",
    "            anal_pred_list.extend(anal_pred)\n",
    "\n",
    "            rmse_list = []\n",
    "            rsquare_list = []\n",
    "            mae_list = []\n",
    "            drmse_list = []\n",
    "        \n",
    "            for i in range(anal_real.shape[0]):\n",
    "                # RMSE\n",
    "                rmse = np.sqrt(mean_squared_error(anal_real[i], anal_pred[i]))\n",
    "                rmse_list.append(rmse)\n",
    "                rmse_data.append(rmse)\n",
    "                # R square\n",
    "                rsquare = r2_score(anal_real[i], anal_pred[i])\n",
    "                rsquare_list.append(rsquare)\n",
    "                # MAE\n",
    "                mae = mean_absolute_error(anal_real[i], anal_pred[i])\n",
    "                mae_list.append(mae)\n",
    "            # RMSE derivative\n",
    "            real_d = anal_real[:,:-1] - anal_real[:,1:]\n",
    "            pred_d = anal_pred[:,:-1] - anal_pred[:,1:]\n",
    "            drmse = np.sqrt(np.sum(((real_d - pred_d) ** 2), axis=1) / anal_real.shape[0])\n",
    "            drmse_list.append(drmse)\n",
    "            # calculate median\n",
    "            rmse_median.append(np.median(rmse_list))\n",
    "            rsquare_median.append(np.median(rsquare_list))\n",
    "            mae_median.append(np.median(mae_list))\n",
    "            drmse_median.append(np.median(drmse_list))\n",
    "\n",
    "        # all test results\n",
    "        anal_index = np.array(anal_index, dtype='int32')\n",
    "        anal_real_list = np.array(anal_real_list, dtype='float32')\n",
    "        anal_pred_list = np.array(anal_pred_list, dtype='float32')\n",
    "\n",
    "\n",
    "        print(\"#####################\")\n",
    "        print(\"##     output results  ##\")\n",
    "        print(\"#####################\")\n",
    "        print(netG)\n",
    "\n",
    "        # output profiles\n",
    "        device_cpu = torch.device('cpu')\n",
    "        exp = pd.DataFrame(anal_real_list)\n",
    "        exp.insert(loc = 0, column='smiles', value= smiles_list[anal_index])\n",
    "        if data_case == 1:\n",
    "            exp.to_csv('{}/exp_profiles_dataset1.csv'.format(out_result))\n",
    "        elif data_case == 2:\n",
    "            exp.to_csv('{}/exp_profiles_dataset2.csv'.format(out_result))\n",
    "        elif data_case == 3:\n",
    "            exp.to_csv('{}/exp_profiles_USGS.csv'.format(out_result))\n",
    "        pred = pd.DataFrame(anal_pred_list)\n",
    "        pred.insert(loc = 0, column='smiles', value= smiles_list[anal_index])\n",
    "        if data_case == 1:\n",
    "            pred.to_csv('{}/pred_profiles_dataset1.csv'.format(out_result))\n",
    "        elif data_case == 2:\n",
    "            pred.to_csv('{}/pred_profiles_dataset2.csv'.format(out_result))\n",
    "        elif data_case == 3:\n",
    "            pred.to_csv('{}/pred_profiles_USGS.csv'.format(out_result))\n",
    "            \n",
    "        # RMSE        \n",
    "        rmse_df = pd.DataFrame([rmse_data])\n",
    "        rmse_df = rmse_df.transpose()\n",
    "        rmse_df.columns = [\"rmse\"]\n",
    "        rmse_sort = rmse_df.sort_values('rmse')\n",
    "        rmse_sort_index = rmse_sort.index.values\n",
    "        print(\"Median of RMSE:\", np.mean(rmse_median), \"std:\", np.std(rmse_median))\n",
    "        # R2\n",
    "        print(\"Median of R square:\", np.mean(rsquare_median), \"std:\", np.std(rsquare_median))\n",
    "        # MAE\n",
    "        print(\"Median of MAE:\", np.mean(mae_median), \"std:\",np.std(mae_median))            \n",
    "        # dRMSE\n",
    "        print(\"Median of RMSE_derivative\", np.mean(drmse_median), \"std:\",np.std(drmse_median))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b07da15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
